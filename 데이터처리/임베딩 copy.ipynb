{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[42 38  0  0]\n",
      " [43 38  0  0]\n",
      " [21  6  0  0]\n",
      " [23 38  0  0]\n",
      " [36  0  0  0]\n",
      " [20  0  0  0]\n",
      " [22  6  0  0]\n",
      " [38 43  0  0]\n",
      " [22 38  0  0]\n",
      " [27 17 38 23]]\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_test_function.<locals>.test_function at 0x00000172446C0EE0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Accuracy: 89.999998\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "# define documents\n",
    "docs = ['Well done!',\n",
    "\t\t'Good work',\n",
    "\t\t'Great effort',\n",
    "\t\t'nice work',\n",
    "\t\t'Excellent!',\n",
    "\t\t'Weak',\n",
    "\t\t'Poor effort!',\n",
    "\t\t'not good',\n",
    "\t\t'poor work',\n",
    "\t\t'Could have done better.']\n",
    "# define class labels\n",
    "labels = array([1,1,1,1,1,0,0,0,0,0])\n",
    "# integer encode the documents\n",
    "vocab_size = 50\n",
    "encoded_docs = [one_hot(d, vocab_size) for d in docs] #one_hot('Well done!', vocab_size) : 원 핫 인코딩은 인덱스로 바꿔주는 것과 같다. \n",
    "#print(encoded_docs)\n",
    "\n",
    "# pad documents to a max length of 4 words\n",
    "max_length = 4\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "print(padded_docs)\n",
    "\n",
    "# define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 8, input_length=max_length))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# summarize the model\n",
    "#print(model.summary())\n",
    "\n",
    "# fit the model\n",
    "model.fit(padded_docs, labels, epochs=50, verbose=0)\n",
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_10 (Embedding)    (None, 20, 8)             80000     \n",
      "                                                                 \n",
      " flatten_6 (Flatten)         (None, 160)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 1)                 161       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 80,161\n",
      "Trainable params: 80,161\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(25000, 20)\n",
      "[  65   16   38 1334   88   12   16  283    5   16 4472  113  103   32\n",
      "   15   16 5345   19  178   32]\n"
     ]
    }
   ],
   "source": [
    "# 코드 6-6 Embedding 층에 사용할 IMDB 데이터 로드하기\n",
    "# 코드 6-7 IMDB 데이터에 Embedding 층과 분류기 사용하기\n",
    "from keras.datasets import imdb\n",
    "from keras import preprocessing\n",
    "\n",
    "# 특성으로 사용할 단어의 수\n",
    "max_features = 10000\n",
    "# 사용할 텍스트의 길이(가장 빈번한 max_features 개의 단어만 사용합니다)\n",
    "maxlen = 20\n",
    "\n",
    "# 정수 리스트로 데이터를 로드합니다.\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "# 리스트를 (samples, maxlen) 크기의 2D 정수 텐서로 변환합니다.\n",
    "x_train = preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense, Embedding\n",
    "\n",
    "model = Sequential()\n",
    "# 나중에 임베딩된 입력을 Flatten 층에서 펼치기 위해 Embedding 층에 input_length를 지정합니다.\n",
    "model.add(Embedding(10000, 8, input_length=maxlen))\n",
    "# Embedding 층의 출력 크기는 (samples, maxlen, 8)가 됩니다.\n",
    "\n",
    "# 3D 임베딩 텐서를 (samples, maxlen * 8) 크기의 2D 텐서로 펼칩니다.\n",
    "model.add(Flatten())\n",
    "\n",
    "# 분류기를 추가합니다.\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.summary()\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_train[0])\n",
    "\n",
    "# history = model.fit(x_train, y_train,\n",
    "#                     epochs=10,\n",
    "#                     batch_size=32,\n",
    "#                     validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코드 6-6 Embedding 층에 사용할 IMDB 데이터 로드하기\n",
    "# 코드 6-7 IMDB 데이터에 Embedding 층과 분류기 사용하기\n",
    "from keras.datasets import imdb\n",
    "from keras import preprocessing\n",
    "\n",
    "# 특성으로 사용할 단어의 수\n",
    "max_features = 10000\n",
    "# 사용할 텍스트의 길이(가장 빈번한 max_features 개의 단어만 사용합니다)\n",
    "maxlen = 20\n",
    "\n",
    "# 정수 리스트로 데이터를 로드합니다.\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "# 리스트를 (samples, maxlen) 크기의 2D 정수 텐서로 변환합니다.\n",
    "x_train = preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense, Embedding\n",
    "\n",
    "model = Sequential()\n",
    "# 나중에 임베딩된 입력을 Flatten 층에서 펼치기 위해 Embedding 층에 input_length를 지정합니다.\n",
    "model.add(Embedding(10000, 8, input_length=maxlen))\n",
    "# Embedding 층의 출력 크기는 (samples, maxlen, 8)가 됩니다.\n",
    "\n",
    "\n",
    "pred=model.predict(x_train)\n",
    "# history = model.fit(x_train, y_train,\n",
    "#                     epochs=10,\n",
    "#                     batch_size=32,\n",
    "#                     validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25000, 20),\n",
       " (25000, 20, 8),\n",
       " array([  65,   16,   38, 1334,   88,   12,   16,  283,    5,   16, 4472,\n",
       "         113,  103,   32,   15,   16, 5345,   19,  178,   32]),\n",
       " array([[ 0.02794695, -0.00461032,  0.02188099, -0.04362736, -0.01263428,\n",
       "         -0.04596486,  0.04977093,  0.01461614],\n",
       "        [ 0.00383754,  0.03542788,  0.01607219, -0.00466157,  0.00557375,\n",
       "         -0.03846597,  0.02838239, -0.03692248],\n",
       "        [ 0.00198686,  0.04498562,  0.01373413, -0.016418  ,  0.02668693,\n",
       "          0.00266587, -0.00541382,  0.03429573],\n",
       "        [-0.01894714, -0.0233646 , -0.01943244, -0.00158536, -0.00425423,\n",
       "         -0.00981084,  0.03597892, -0.03310579],\n",
       "        [-0.03843913, -0.04508629, -0.00049255, -0.03719985,  0.03546747,\n",
       "         -0.03861538, -0.02566456, -0.04105256],\n",
       "        [-0.00932988,  0.00795971,  0.03784258,  0.03422972,  0.00903815,\n",
       "         -0.03357217,  0.02597134,  0.02618445],\n",
       "        [ 0.00383754,  0.03542788,  0.01607219, -0.00466157,  0.00557375,\n",
       "         -0.03846597,  0.02838239, -0.03692248],\n",
       "        [-0.04678073,  0.00078262,  0.03379519,  0.00113078,  0.01885355,\n",
       "          0.0316794 , -0.01920774,  0.04366131],\n",
       "        [-0.0352985 , -0.03936272,  0.02199199, -0.03618337, -0.01591737,\n",
       "          0.02926371, -0.03122729, -0.04878705],\n",
       "        [ 0.00383754,  0.03542788,  0.01607219, -0.00466157,  0.00557375,\n",
       "         -0.03846597,  0.02838239, -0.03692248],\n",
       "        [-0.0167345 ,  0.03150121,  0.03342135,  0.0314824 , -0.04345553,\n",
       "          0.04642952, -0.03309755,  0.04902736],\n",
       "        [-0.03247178, -0.00816391, -0.00135194,  0.02073356,  0.03056197,\n",
       "          0.034346  ,  0.03692451,  0.03841014],\n",
       "        [-0.00563574, -0.04943143, -0.00431347,  0.01914058,  0.03833488,\n",
       "          0.03713908, -0.01824803,  0.00218993],\n",
       "        [ 0.03326226, -0.03809004,  0.00060846, -0.04683227, -0.01419634,\n",
       "         -0.00167227, -0.00218227,  0.00745485],\n",
       "        [-0.00427244,  0.00247264,  0.03004347,  0.04276982,  0.03013678,\n",
       "         -0.00128897,  0.02183661,  0.02279672],\n",
       "        [ 0.00383754,  0.03542788,  0.01607219, -0.00466157,  0.00557375,\n",
       "         -0.03846597,  0.02838239, -0.03692248],\n",
       "        [ 0.01464916, -0.02069811, -0.0173253 ,  0.02487855,  0.00422024,\n",
       "          0.0415992 ,  0.00252169,  0.04177019],\n",
       "        [-0.00855818, -0.02865022, -0.04910756,  0.04876408,  0.04245151,\n",
       "         -0.04941676, -0.01862668, -0.01360978],\n",
       "        [-0.0296057 , -0.03957262,  0.00041399,  0.04223568, -0.01134038,\n",
       "         -0.01501977,  0.03818817,  0.00782719],\n",
       "        [ 0.03326226, -0.03809004,  0.00060846, -0.04683227, -0.01419634,\n",
       "         -0.00167227, -0.00218227,  0.00745485]], dtype=float32))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, pred.shape, x_train[0], pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5812e37fbc42ae0019c075dcd625ea6adf837b197758e07cfdfe5b415c77a600"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
