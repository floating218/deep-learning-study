{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 요약\n",
    "## 공통  \n",
    "- `__init__(self, **kwargs)`\n",
    "- `get_config(self)`   \n",
    "## 사용자 정의 손실 함수  \n",
    "- `keras.losses.Loss`를 상속받으며, `call(self,y_true,y_pred)`에서 새로운 loss 결과를 리턴한다.  \n",
    "- `model.compile(loss=Loss클래스(), optimizer=..., metrics=[...])`    \n",
    "## 사용자 정의 활성화함수(activation) / 초기화(initializer) / 규제(regularizer) / 제한(constraint)   \n",
    "- `model.layers.Dense(안에, activation='', kernel_regularizer='', kernel_initializer='', kernel_constraint='')`\n",
    "- initializer함수: shape를 받아서 초기화하는 함수\n",
    "- regularizer함수: weights을 받아서, 원래의 Loss에 더해지는 값\n",
    "- constraint함수: weights를 받아서, weights와 같은 차원을 값으로 변형해서 리턴해주는 함수  \n",
    "## 사용자 정의 metric 함수  \n",
    "- `keras.metrics.Metric`을 상속받으며, `update_state(self,y_true,y_pred)` -> `result(self)`순서로 호출\n",
    "- `result(self)`에 새로운 metric 결과를 리턴한다\n",
    "- `model.complie(loss=..., optimizer=..., weighted_metrics=[Metric클래스()])`  \n",
    "## 사용자 지정 레이어\n",
    "- `keras.layers.Layer`를 상속 받음\n",
    "- `build()`: 가중치마다 add_weights() 메서드를 호출하여 층의 변수를 만든다. 층이 처음 사용될 때 호출된다. 필수 메서드는 아니다.\n",
    "- `call()`: 층에 필요한 연산을 수행한다. 활성화 함수까지 들어간다. 이 값이 층의 출력에 해당한다. \n",
    "- `call()`안에 `training=None` 옵션을 넣으면 학습과 검증을 다르게 할 수 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파이썬 ≥3.5 필수\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# 사이킷런 ≥0.20 필수\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "try:\n",
    "    # %tensorflow_version은 코랩 명령입니다.\n",
    "    %tensorflow_version 2.x\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# 이 노트북은 텐서플로 ≥2.4이 필요합니다\n",
    "# 2.x 버전은 대부분 동일한 결과를 만들지만 몇 가지 버그가 있습니다.\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "assert tf.__version__ >= \"2.4\"\n",
    "\n",
    "# 공통 모듈 임포트\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# 노트북 실행 결과를 동일하게 유지하기 위해\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# 깔끔한 그래프 출력을 위해\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# 그림을 저장할 위치\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"deep\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"그림 저장:\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    housing.data, housing.target.reshape(-1, 1), random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_full, y_train_full, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 사용자 정의 손실함수(Loss)\n",
    "- `keras.losses.Loss`를 상속\n",
    "- `def __init__(self, **kwargs)`: 어떤 매개변수를 받아서 전달할지를 결정하는 함수. \n",
    "  - `super().__init__(**kwargs)`\n",
    "- `def call(self, y_true, y_pred)`: 레이블과 예측을 받고 모든 샘플의 손실을 계산하여 반환하는 함수.\n",
    "- `def get_config(self)`: 매개변수를 딕셔너리로 만들어서 반환하는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.7864 - mae: 0.9470 - val_loss: 0.4276 - val_mae: 0.6122\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 0s 829us/step - loss: 0.2497 - mae: 0.5204 - val_loss: 0.3437 - val_mae: 0.5537\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23790757e50>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class HuberLoss(keras.losses.Loss):\n",
    "    def __init__(self, threshold=1.0, **kwargs):\n",
    "        self.threshold = threshold\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "    def call(self, y_true, y_pred):\n",
    "        error = y_true - y_pred #차이를 계산\n",
    "        is_small_error = tf.abs(error) < self.threshold #차이가 역치보다 작음\n",
    "        squared_loss = tf.square(error) / 2 #차이의 제곱근 나누기 2\n",
    "        linear_loss  = self.threshold * tf.abs(error) - self.threshold**2 / 2 #역치*차이-역치^2\n",
    "        return tf.where(is_small_error, squared_loss, linear_loss) #tf.where(차이가 작은가?, 작을경우 차이의 제곱근 나누기 2, 클경우 역치*차이-역치^2)\n",
    "    \n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"threshold\": self.threshold} #매개변수를 딕셔너리로 만들어서 반환하는 함수\n",
    "\n",
    "input_shape = X_train.shape[1:]\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n",
    "                       input_shape=input_shape),\n",
    "    keras.layers.Dense(1),\n",
    "])\n",
    "\n",
    "keras.utils.plot_model(model, \"-.png\", show_shapes=True)\n",
    "\n",
    "model.compile(loss=HuberLoss(2.), optimizer=\"nadam\", metrics=[\"mae\"])\n",
    "\n",
    "model.fit(X_train_scaled, y_train, epochs=2,\n",
    "          validation_data=(X_valid_scaled, y_valid))\n",
    "\n",
    "# model.save(\"my_model_with_a_custom_loss_class.h5\")\n",
    "\n",
    "# model = keras.models.load_model(\"my_model_with_a_custom_loss_class.h5\",\n",
    "#                                 custom_objects={\"HuberLoss\": HuberLoss})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 사용자 정의 활성화함수(activation) / 초기화(initializer) / 규제(regularizer) / 제한(constraint) \n",
    "\n",
    "- `activation`: keras.activatons 참고\n",
    "  - 일반예시: `softmax, elu, selu, softplus, softsign, relu, tanh, sigmoid, linear`  \n",
    "\n",
    "- `kernel_initializer`\n",
    "  - 일반예시: `\"he_normal\"`, `\"glorot_uniform\"`\n",
    "  - shape를 인풋으로 받아 shape에 맞게끔 최초 값이 초기화되어 리턴되는 함수가 들어간다.   \n",
    "\n",
    "- `kernel_regularizer`\n",
    "  - https://light-tree.tistory.com/125 \n",
    "  - 일반예시: `keras.regularizer.l1(0.01)`, `keras.regularizer.l2(0.01)`  \n",
    "  - 기존의 Loss 결과(Loss(y_true,y_pred))에 더해지는 수치 함수가 들어간다.   \n",
    "\n",
    "- `kernel_constraints`\n",
    "  - 일반예시: `keras.constraints.max_norm(1.)`\n",
    "  - 기존의 weights에 변형을 가한 결과 함수가 들어간다.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "363/363 [==============================] - 1s 983us/step - loss: 1.5542 - mae: 0.8962 - val_loss: 1.4154 - val_mae: 0.5607\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 0s 788us/step - loss: 0.5943 - mae: 0.5256 - val_loss: 1.4399 - val_mae: 0.5137\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2379abaa880>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def my_softplus(z): # tf.nn.softplus(z) 값을 반환합니다\n",
    "    return tf.math.log(tf.exp(z) + 1.0)\n",
    "\n",
    "def my_glorot_initializer(shape, dtype=tf.float32):\n",
    "    stddev = tf.sqrt(2. / (shape[0] + shape[1]))\n",
    "    return tf.random.normal(shape, stddev=stddev, dtype=dtype)\n",
    "\n",
    "def my_l1_regularizer(weights):\n",
    "    return tf.reduce_sum(tf.abs(0.01 * weights))\n",
    "\n",
    "def my_positive_weights(weights): # tf.nn.relu(weights) 값을 반환합니다\n",
    "    return tf.where(weights < 0., tf.zeros_like(weights), weights)\n",
    "\n",
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n",
    "                       input_shape=input_shape),\n",
    "    keras.layers.Dense(1, activation=my_softplus,\n",
    "                       kernel_regularizer=my_l1_regularizer,\n",
    "                       kernel_constraint=my_positive_weights,\n",
    "                       kernel_initializer=my_glorot_initializer),\n",
    "])\n",
    "model.compile(loss=\"mse\", optimizer=\"nadam\", metrics=[\"mae\"])\n",
    "model.fit(X_train_scaled, y_train, epochs=2,\n",
    "          validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 사용자 정의 지표(Metric)\n",
    "\n",
    "- Loss처럼 `__init__()`과 `get_config()`를 가지고 있음\n",
    "- Loss의 `call()`대신 `update_state()`, `result()` \n",
    "- `update_state()`: 함수처럼 사용할 때 호출된다. \n",
    "  - y_true, y_pred를 받아서 metric을 계산\n",
    "  - `metric = self.huber_fn(y_true, y_pred)`\n",
    "  - `self.total.assign_add(tf.reduce_sum(metric))`\n",
    "- `result()` 최종 결과를 계산하고 반환함. 호출 순서는 update_state -> result()순서"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "363/363 [==============================] - 1s 631us/step - loss: 0.8707 - huber_metric: 0.8707\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 0s 603us/step - loss: 0.2595 - huber_metric: 0.2595\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2379066e7c0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_huber(threshold=1.0):\n",
    "    def huber_fn(y_true, y_pred):\n",
    "        error = y_true - y_pred\n",
    "        is_small_error = tf.abs(error) < threshold\n",
    "        squared_loss = tf.square(error) / 2\n",
    "        linear_loss  = threshold * tf.abs(error) - threshold**2 / 2\n",
    "        return tf.where(is_small_error, squared_loss, linear_loss)\n",
    "    return huber_fn\n",
    "\n",
    "class HuberMetric(keras.metrics.Metric):\n",
    "    def __init__(self, threshold=1.0, **kwargs):\n",
    "        super().__init__(**kwargs) # 기본 매개변수 처리 (예를 들면, dtype)\n",
    "        self.threshold = threshold\n",
    "        self.huber_fn = create_huber(threshold)\n",
    "        self.total = self.add_weight(\"total\", initializer=\"zeros\")#상태변수 추가하는 함수\n",
    "        self.count = self.add_weight(\"count\", initializer=\"zeros\")\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        metric = self.huber_fn(y_true, y_pred)\n",
    "        self.total.assign_add(tf.reduce_sum(metric))\n",
    "        self.count.assign_add(tf.cast(tf.size(y_true), tf.float32))\n",
    "    def result(self):\n",
    "        return self.total / self.count\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"threshold\": self.threshold} \n",
    "\n",
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n",
    "                       input_shape=input_shape),\n",
    "    keras.layers.Dense(1),\n",
    "])\n",
    "\n",
    "model.compile(loss=keras.losses.Huber(2.0), optimizer=\"nadam\", \n",
    "              weighted_metrics=[HuberMetric(2.0)])\n",
    "model.fit(X_train_scaled.astype(np.float32), y_train.astype(np.float32), epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 사용자 지정 레이어\n",
    "- `keras.layers.Layer`를 상속 받음\n",
    "- `build()`: 가중치마다 add_weights() 메서드를 호출하여 층의 변수를 만든다. 층이 처음 사용될 때 호출된다. 필수 메서드는 아니다.\n",
    "- `call()`: 층에 필요한 연산을 수행한다. 활성화 함수까지 들어간다. 이 값이 층의 출력에 해당한다. \n",
    "- `call()`안에 `training=None` 옵션을 넣으면 학습과 검증을 다르게 할 수 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "363/363 [==============================] - 1s 988us/step - loss: 4.6220 - val_loss: 1.1015\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 0s 788us/step - loss: 2.5151 - val_loss: 1.0572\n",
      "162/162 [==============================] - 0s 465us/step - loss: 0.7047\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7047481536865234"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyDense(keras.layers.Layer):\n",
    "    def __init__(self, units, activation=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.activation = keras.activations.get(activation)\n",
    "\n",
    "    def build(self, batch_input_shape):\n",
    "        self.kernel = self.add_weight(\n",
    "            name=\"kernel\", \n",
    "            shape=[batch_input_shape[-1], self.units],\n",
    "            initializer=\"glorot_normal\")\n",
    "        self.bias = self.add_weight(\n",
    "            name=\"bias\", \n",
    "            shape=[self.units], \n",
    "            initializer=\"zeros\")\n",
    "        super().build(batch_input_shape) # must be at the end\n",
    "\n",
    "    def call(self, X, training=None):\n",
    "        result = self.activation(X @ self.kernel + self.bias)\n",
    "        if training:\n",
    "            stddev=1.0\n",
    "            noise = tf.random.normal(tf.shape(result), stddev=stddev)\n",
    "            return result + noise\n",
    "        return result\n",
    "\n",
    "    def compute_output_shape(self, batch_input_shape):\n",
    "        return tf.TensorShape(batch_input_shape.as_list()[:-1] + [self.units])\n",
    "\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"units\": self.units,\n",
    "                \"activation\": keras.activations.serialize(self.activation)}\n",
    "        \n",
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    MyDense(30, activation=\"relu\", input_shape=input_shape),\n",
    "    MyDense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss=\"mse\", optimizer=\"nadam\")\n",
    "model.fit(X_train_scaled, y_train, epochs=2,\n",
    "          validation_data=(X_valid_scaled, y_valid))\n",
    "model.evaluate(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "363/363 [==============================] - 1s 930us/step - loss: 2.3857 - val_loss: 7.6082\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 0s 752us/step - loss: 1.0571 - val_loss: 4.4597\n",
      "162/162 [==============================] - 0s 497us/step - loss: 0.7560\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7559615969657898"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#사용자가 정의한 레이어\n",
    "class AddGaussianNoise(keras.layers.Layer):\n",
    "    def __init__(self, stddev, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.stddev = stddev\n",
    "\n",
    "    def call(self, X, training=None):\n",
    "        if training:\n",
    "            noise = tf.random.normal(tf.shape(X), stddev=self.stddev)\n",
    "            return X + noise\n",
    "        else:\n",
    "            return X\n",
    "\n",
    "    def compute_output_shape(self, batch_input_shape):\n",
    "        return batch_input_shape\n",
    "    \n",
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    AddGaussianNoise(stddev=1.0),\n",
    "    keras.layers.Dense(30, activation=\"selu\"),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss=\"mse\", optimizer=\"nadam\")\n",
    "model.fit(X_train_scaled, y_train, epochs=2,\n",
    "          validation_data=(X_valid_scaled, y_valid))\n",
    "model.evaluate(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 사용자 정의 모델\n",
    "- 복잡하니까 나중에 정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "363/363 [==============================] - 1s 769us/step - loss: 0.7885 - reconstruction_error: 0.0000e+00\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 0s 769us/step - loss: 0.4126 - reconstruction_error: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "class ReconstructingRegressor(keras.models.Model):\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden = [keras.layers.Dense(30, activation=\"selu\",\n",
    "                                          kernel_initializer=\"lecun_normal\")\n",
    "                       for _ in range(5)]\n",
    "        self.out = keras.layers.Dense(output_dim)\n",
    "        self.reconstruct = keras.layers.Dense(8) # TF 이슈 #46858에 대한 대책\n",
    "        self.reconstruction_mean = keras.metrics.Mean(name=\"reconstruction_error\")\n",
    "\n",
    "#     TF 이슈 #46858 때문에 주석 처리\n",
    "#     def build(self, batch_input_shape):\n",
    "#         n_inputs = batch_input_shape[-1]\n",
    "#         self.reconstruct = keras.layers.Dense(n_inputs, name='recon')\n",
    "#         super().build(batch_input_shape)\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        Z = inputs\n",
    "        for layer in self.hidden:\n",
    "            Z = layer(Z)\n",
    "        reconstruction = self.reconstruct(Z)\n",
    "        self.recon_loss = 0.05 * tf.reduce_mean(tf.square(reconstruction - inputs))\n",
    "        \n",
    "        if training:\n",
    "           result = self.reconstruction_mean(recon_loss)\n",
    "           self.add_metric(result)\n",
    "        return self.out(Z)\n",
    "    \n",
    "    def train_step(self, data):\n",
    "        x, y = data\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self(x)\n",
    "            loss = self.compiled_loss(y, y_pred, regularization_losses=[self.recon_loss])\n",
    "\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "    \n",
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "model = ReconstructingRegressor(1)\n",
    "model.compile(loss=\"mse\", optimizer=\"nadam\")\n",
    "history = model.fit(X_train_scaled, y_train, epochs=2)\n",
    "y_pred = model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4d97dfac72b5344da9592506e8ae21cbbdfd9437ca0e05329aac99300b906c4a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
